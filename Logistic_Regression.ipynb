{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_betas(dim):\n",
    "    # uniform initialization uniform sample points from a uniform distribution from 0 to 1.\n",
    "    #b = np.random.random()\n",
    "    #w = np.random.random(dim)\n",
    "    #Gaussian Initialization gaussian samples points from a normal distribution with mean 0 and std 1.\n",
    "    b = np.random.randn()\n",
    "    w = np.random.randn(dim)\n",
    "    return b,w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost( y, y_hat):\n",
    "      #return np.sum(np.dot(y.T,np.log(1-y_hat)+ np.dot((1-y).T,np.log(1-y_hat)))) / ( len(y))\n",
    "        return - np.sum(np.dot(y.T,np.log(1-y_hat)+ np.dot((1-y).T,np.log(1-y_hat)))) / ( len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(b, w ,X_new):\n",
    "    Z = b + np.matmul(X_new,w)\n",
    "    return (1.0 / (1 + np.exp(-Z)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta (b_0, w_0 , y , y_hat, X_new, alpha):\n",
    "    db = np.sum( y_hat - y)/ len(y)\n",
    "    b_0 = b_0 - alpha * db\n",
    "    dw = np.dot((y_hat - y), X_new)/ len(y)\n",
    "    w_0 = w_0 - alpha * dw\n",
    "   \n",
    "   \n",
    "    return b_0,w_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta_L1 (b_0, w_0 , y , y_hat, X_new, alpha,lamba):\n",
    "    db = np.sum( y_hat - y)/ len(y)\n",
    "    b_0 = b_0 - alpha * db\n",
    "    #print\n",
    "    dw = (np.dot((y_hat - y), X_new)+(lamba/2)*np.sign(w_0))/ len(y)\n",
    "    #print(\"dw\",dw)\n",
    "    w_0 = w_0 - alpha * dw\n",
    "   \n",
    "   \n",
    "    return b_0,w_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_beta_L2 (b_0, w_0 , y , y_hat, X_new, alpha,lamba):\n",
    "    db = np.sum( y_hat - y)/ len(y)\n",
    "    b_0 = b_0 - alpha * db\n",
    "    #print\n",
    "    dw = (np.dot((y_hat - y), X_new)+lamba*(w_0))/ len(y)\n",
    "    #print(\"dw\",dw)\n",
    "    w_0 = w_0 - alpha * dw\n",
    "   \n",
    "   \n",
    "    return b_0,w_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X_new,Y,alpha):\n",
    "    num_iterations = 0\n",
    "    \n",
    "    #alpha = 0.5\n",
    "    all_costs = []\n",
    "    b,w = initialize_betas(X_new.shape[1])\n",
    "    initial_b,initial_w = b,w\n",
    "    print(\"initial guess of b and w: \" , b ,w)\n",
    "    \n",
    "    difference = 100\n",
    "    while difference > 0.0001 :\n",
    "        \n",
    "        if(num_iterations>1):\n",
    "            difference = all_costs[len(all_costs)-2] - all_costs[len(all_costs)-1]\n",
    "        \n",
    "    #for each_iter in range (num_iterations ):\n",
    "        num_iterations = num_iterations + 1\n",
    "        \n",
    "        y_hat = sigmoid(b, w , X_new)\n",
    "        current_cost = get_cost (Y, y_hat)\n",
    "        prev_b = b\n",
    "        prev_w = w\n",
    "        b, w = update_beta (prev_b, prev_w, Y, y_hat, X_new, alpha)\n",
    "        all_costs.append(current_cost)\n",
    "       # if num_iterations % 1000 == 0:\n",
    "        #    print('Iteration: ', num_iterations, 'Cost: ', current_cost)\n",
    "            \n",
    "        \n",
    "    #print('b_0:', b_0, 'b_1:',b_1,'b_2:',b_2,'b_3:',b_3,'b_4:', b_4, 'b_5:',b_5,'b_6:',b_6,'b_7:',b_7,'b_8:',b_8,'b_9:',b_9)\n",
    "    print(\"Learning rate : \",alpha)\n",
    "    print(\"Final estimates of b and q are: \", b,w)\n",
    "    print(\"Number of Iterations : \",num_iterations);\n",
    "    #plt.scatter( range(num_iterations), all_costs)\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.title(f'Cost Function vs iteration plot \\n Initial_weights ={initial_b,initial_w}\\n alpha={alpha} ')\n",
    "    #plt.xlabel(\"num_iterations\")\n",
    "    #plt.ylabel(\"current_cost\")\n",
    "    #plt.plot(all_costs,c='c',label='training set avg cost')\n",
    "   # plt.plot(model.cost_arr['test'], c='r',label='testing set avg cost')\n",
    "    #plt.legend(loc='upper right')\n",
    "    #plt.savefig(f\".{alpha}_{max_iter}_{batch_size}_{Layers[1:3]}_cost_iter_iter_{iterat}.png\")\n",
    "    #plt.show()\n",
    "    \n",
    "    return b,w\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_gradient(X_new,Y,alpha):\n",
    "    num_iterations = 0\n",
    "    \n",
    "    #alpha = 0.5\n",
    "    all_costs = []\n",
    "    b,w = initialize_betas(X_new.shape[1])\n",
    "    initial_b,initial_w = b,w\n",
    "    print(\"initial guess of b and w: \" , b ,w)\n",
    "    \n",
    "    difference = 100\n",
    "    lamba = 0.01\n",
    "    while difference > 0.000001 :\n",
    "        \n",
    "        if(num_iterations>1):\n",
    "            difference = all_costs[len(all_costs)-2] - all_costs[len(all_costs)-1]\n",
    "        \n",
    "    #for each_iter in range (num_iterations ):\n",
    "        num_iterations = num_iterations + 1\n",
    "        \n",
    "        y_hat = sigmoid(b, w , X_new)\n",
    "        current_cost = get_cost (Y, y_hat) + lamba*np.sum(np.absolute(w))/2*len(Y)\n",
    "        prev_b = b\n",
    "        prev_w = w\n",
    "        b, w = update_beta_L1 (prev_b, prev_w, Y, y_hat, X_new, alpha,lamba)\n",
    "        all_costs.append(current_cost)\n",
    "        #print(current_cost)\n",
    "        #if num_iterations % 1000 == 0:\n",
    "            #print('Iteration: ', num_iterations, 'Cost: ', current_cost)\n",
    "            \n",
    "        \n",
    "    #print('b_0:', b_0, 'b_1:',b_1,'b_2:',b_2,'b_3:',b_3,'b_4:', b_4, 'b_5:',b_5,'b_6:',b_6,'b_7:',b_7,'b_8:',b_8,'b_9:',b_9)\n",
    "    \n",
    "    print(\"Learning rate : \",alpha)\n",
    "    print(\"Final estimates of b and q are: \", b,w)\n",
    "    print(\"Number of Iterations : \",num_iterations);\n",
    "    #plt.scatter( range(num_iterations), all_costs)\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.title(f'Cost Function vs iteration plot lambda={lamba} \\n Initial_weights ={initial_b,initial_w}\\n alpha={alpha} ')\n",
    "    #plt.xlabel(\"num_iterations\")\n",
    "    #plt.ylabel(\"current_cost\")\n",
    "    #plt.plot(all_costs,c='c',label='training set avg cost')\n",
    "   # plt.plot(model.cost_arr['test'], c='r',label='testing set avg cost')\n",
    "    #plt.legend(loc='upper right')\n",
    "    #plt.savefig(f\".{alpha}_{max_iter}_{batch_size}_{Layers[1:3]}_cost_iter_iter_{iterat}.png\")\n",
    "    #plt.show()\n",
    "    return b,w\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_gradient(X_new,Y,alpha):\n",
    "    num_iterations = 0\n",
    "    \n",
    "    #alpha = 0.5\n",
    "    all_costs = []\n",
    "    b,w = initialize_betas(X_new.shape[1])\n",
    "    initial_b,initial_w = b,w\n",
    "    print(\"initial guess of b and w: \" , b ,w)\n",
    "    \n",
    "    difference = 100\n",
    "    lamba = 0.01\n",
    "    while difference > 0.000001 :\n",
    "        \n",
    "        if(num_iterations>1):\n",
    "            difference = all_costs[len(all_costs)-2] - all_costs[len(all_costs)-1]\n",
    "        \n",
    "    #for each_iter in range (num_iterations ):\n",
    "        num_iterations = num_iterations + 1\n",
    "        \n",
    "        y_hat = sigmoid(b, w , X_new)\n",
    "        current_cost = get_cost (Y, y_hat) + lamba*np.sum(np.square(w))/2*len(Y)\n",
    "        prev_b = b\n",
    "        prev_w = w\n",
    "        b, w = update_beta_L2 (prev_b, prev_w, Y, y_hat, X_new, alpha,lamba)\n",
    "        all_costs.append(current_cost)\n",
    "        #print(current_cost)\n",
    "        if num_iterations % 1000 == 0:\n",
    "            #print('Iteration: ', num_iterations, 'Cost: ', current_cost)\n",
    "            pass  \n",
    "            \n",
    "        \n",
    "    #print('b_0:', b_0, 'b_1:',b_1,'b_2:',b_2,'b_3:',b_3,'b_4:', b_4, 'b_5:',b_5,'b_6:',b_6,'b_7:',b_7,'b_8:',b_8,'b_9:',b_9)\n",
    "    print(\"Learning rate : \",alpha)\n",
    "    print(\"Final estimates of b and q are: \", b,w)\n",
    "    print(\"Number of Iterations : \",num_iterations);\n",
    "    #plt.scatter( range(num_iterations), all_costs)\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.title(f'Cost Function vs iteration plot lambda ={lamba} \\n Initial_weights ={initial_b,initial_w}\\n alpha={alpha} ')\n",
    "    #plt.xlabel(\"num_iterations\")\n",
    "    #plt.ylabel(\"current_cost\")\n",
    "    #plt.plot(all_costs,c='c',label='training set avg cost')\n",
    "   # plt.plot(model.cost_arr['test'], c='r',label='testing set avg cost')\n",
    "    #plt.legend(loc='upper right')\n",
    "    #plt.savefig(f\".{alpha}_{max_iter}_{batch_size}_{Layers[1:3]}_cost_iter_iter_{iterat}.png\")\n",
    "    #plt.show()\n",
    "    return b,w\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test,Y_test,b,w):\n",
    "    y_hat_test = sigmoid(b,w,X_test)\n",
    "    Y_predict=[]\n",
    "    tp,tn,fp,fn=0,0,0,0\n",
    "    for i in y_hat_test:\n",
    "        if(i>0.5): \n",
    "            num = 1\n",
    "            Y_predict.append(num)\n",
    "        elif(i<0.5):\n",
    "            num = 0\n",
    "            Y_predict.append(num)\n",
    "    for i in range(len(Y_test)):\n",
    "        if(Y_test[i]==Y_predict[i]):\n",
    "            if(Y_test[i]):\n",
    "                tp=tp+1\n",
    "            else:\n",
    "                tn = tn+1\n",
    "        else:\n",
    "            if(Y_test[i]):\n",
    "                fn = fn+1\n",
    "            else:\n",
    "                fp = fp+1\n",
    "    #print(tp,tn,fp,fn)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    print(\"Accuracy : \",accuracy)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    print(\"Precision : \",precision,\"\\nrecall :\",recall)\n",
    "    F1_score = 2*precision*recall/(precision+recall)\n",
    "    print(\"F1_score :\",F1_score)\n",
    "    \n",
    "    #print('/////////////////////////////////////////////////////////////////////////////////////////////////////////')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data2 = pd.read_csv('1.txt',sep= \",\",header=None)\n",
    "    col=data2.shape[1]\n",
    "    \n",
    "    X = data2.iloc[:,0:col-1] # features dataframe\n",
    "    Y = data2.iloc[:,col-1:col] # label dataframe\n",
    "    \n",
    "    # Normalizing the features\n",
    "    sc_X = StandardScaler()\n",
    "    X_new = sc_X.fit_transform(X)\n",
    "    X_new = pd.DataFrame(data=X_new)\n",
    "    #Convert dataframe to array\n",
    "    X_new1 = np.array(X_new)\n",
    "    #print(type(X))\n",
    "    #mean and std of normalized features\n",
    "    #print(\"Mean and Standard Deviation of preprocessed data\",X_new1.mean(axis=0),X_new1.std(axis=0))\n",
    "    \n",
    "    #convert dataframe to array\n",
    "    Y1 = np.array(Y)\n",
    "    Y1=Y1.reshape(len(Y1), )\n",
    "    \n",
    "    # splitting the data - traindata[80%] and test_data[20%]\n",
    "    indices = np.random.permutation(X_new1.shape[0])\n",
    "    indices1 = list(range(X_new1.shape[0]))\n",
    "    num_training_instances = int(0.8 * X_new1.shape[0])\n",
    "    train_indices = indices1[:num_training_instances]\n",
    "    test_indices = indices1[num_training_instances:]\n",
    "        \n",
    "    X_data_train, X_data_test = np.array(X_new.iloc[train_indices]), np.array(X_new.iloc[test_indices])\n",
    "    Y_data_train, Y_data_test = np.array(Y.iloc[train_indices]), np.array(Y.iloc[test_indices])\n",
    "    \n",
    "    Y_data_train = Y_data_train.reshape(len(Y_data_train),)\n",
    "    \n",
    "    # list of different learning rates\n",
    "    learning_rate = [0.01,0.5,1,10]\n",
    "    for alpha in learning_rate:\n",
    "        print(\"//////////////////////////// GRADIENT   DESCENT///////////////////////////\")\n",
    "        b,w=gradient(X_data_train,Y_data_train,alpha)\n",
    "        predict(X_data_test,Y_data_test,b,w)\n",
    "        print(\"//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\")\n",
    "        b,w = L1_gradient(X_data_train,Y_data_train,alpha)\n",
    "        predict(X_data_test,Y_data_test,b,w)\n",
    "        print(\"//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\")\n",
    "        b,w = L2_gradient(X_data_train,Y_data_train,alpha)\n",
    "        predict(X_data_test,Y_data_test,b,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_main():\n",
    "    data2 = pd.read_csv('1.txt',sep= \",\",header=None)\n",
    "    col=data2.shape[1]\n",
    "    \n",
    "    X = data2.iloc[:,0:col-2]# features dataframe\n",
    "    Y = data2.iloc[:,col-1:col]  # label dataframe\n",
    "    \n",
    "    # Normalizing the features\n",
    "    sc_X = StandardScaler()\n",
    "    X_new = sc_X.fit_transform(X)\n",
    "    X_new = pd.DataFrame(data=X_new)\n",
    "    #Convert dataframe to array\n",
    "    X_new1 = np.array(X_new)\n",
    "    \n",
    "    #mean and std of normalized features\n",
    "    #print(X_new1.mean(axis=0),X_new1.std(axis=0))\n",
    "    \n",
    "    Y1 = np.array(Y)\n",
    "    Y1=Y1.reshape(len(Y1), )\n",
    "    \n",
    "    # splitting the data - traindata[80%] and test_data[20%]\n",
    "    indices = np.random.permutation(X_new1.shape[0])\n",
    "    indices1 = list(range(X_new1.shape[0]))\n",
    "    num_training_instances = int(0.8 * X_new1.shape[0])\n",
    "    train_indices = indices1[:num_training_instances]\n",
    "    test_indices = indices1[num_training_instances:]\n",
    "        \n",
    "    X_data_train, X_data_test = np.array(X_new.iloc[train_indices]), np.array(X_new.iloc[test_indices])\n",
    "    Y_data_train, Y_data_test = np.array(Y.iloc[train_indices]), np.array(Y.iloc[test_indices])\n",
    "    \n",
    "    Y_data_train = Y_data_train.reshape(len(Y_data_train),)\n",
    "    learning_rate = [0.01,0.5,1,10]\n",
    "    \n",
    "    # list of different learning rates\n",
    "    for alpha in learning_rate:\n",
    "        print(\"//////////////////////////// GRADIENT   DESCENT///////////////////////////\")\n",
    "        b,w=gradient(X_data_train,Y_data_train,alpha)\n",
    "        predict(X_data_test,Y_data_test,b,w)\n",
    "        print(\"//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\")\n",
    "        b,w = L1_gradient(X_data_train,Y_data_train,alpha)\n",
    "        predict(X_data_test,Y_data_test,b,w)\n",
    "        print(\"//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\")\n",
    "        b,w = L2_gradient(X_data_train,Y_data_train,alpha)\n",
    "        predict(X_data_test,Y_data_test,b,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////// GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -0.4919238279470484 [ 1.1947718  -0.09440119 -0.06146068  0.0866962 ]\n",
      "Learning rate :  0.01\n",
      "Final estimates of b and q are:  -1.5796287176413395 [-4.60886088 -4.35495202 -4.0630893   0.35787192]\n",
      "Number of Iterations :  30746\n",
      "Accuracy :  0.9927272727272727\n",
      "Precision :  1.0 \n",
      "recall : 0.9927272727272727\n",
      "F1_score : 0.9963503649635036\n",
      "//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  1.538913481606267 [ 0.5218812   1.1191954  -0.35750724  0.09841795]\n",
      "Learning rate :  0.01\n",
      "Final estimates of b and q are:  -0.7616100384000506 [-2.3331009  -1.40614447 -1.11274565  0.21251046]\n",
      "Number of Iterations :  2917\n",
      "Accuracy :  0.9127272727272727\n",
      "Precision :  1.0 \n",
      "recall : 0.9127272727272727\n",
      "F1_score : 0.9543726235741445\n",
      "//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.8030324827096792 [ 1.07307139  2.40897337  0.89994929 -1.37533115]\n",
      "Learning rate :  0.01\n",
      "Final estimates of b and q are:  -0.757356274657059 [-1.78158736 -0.74297454 -0.17572196  0.05764916]\n",
      "Number of Iterations :  2228\n",
      "Accuracy :  0.7963636363636364\n",
      "Precision :  1.0 \n",
      "recall : 0.7963636363636364\n",
      "F1_score : 0.8866396761133604\n",
      "//////////////////////////// GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -0.15339583939416263 [ 1.05665211 -0.7907413   1.41218336 -0.52249036]\n",
      "Learning rate :  0.5\n",
      "Final estimates of b and q are:  -4.020364671333154 [-8.62544362 -9.31698432 -8.52356437  0.13199725]\n",
      "Number of Iterations :  7552\n",
      "Accuracy :  0.9927272727272727\n",
      "Precision :  1.0 \n",
      "recall : 0.9927272727272727\n",
      "F1_score : 0.9963503649635036\n",
      "//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -0.9720288108244326 [-0.60927187  0.51342603  2.17707084  1.03702352]\n",
      "Learning rate :  0.5\n",
      "Final estimates of b and q are:  -0.855833445608594 [-2.43649371 -0.88222953 -0.70954211  0.4253988 ]\n",
      "Number of Iterations :  59\n",
      "Accuracy :  0.8654545454545455\n",
      "Precision :  1.0 \n",
      "recall : 0.8654545454545455\n",
      "F1_score : 0.9278752436647174\n",
      "//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.2017609745474399 [-0.34263926 -0.3011656   0.63891226 -0.36726363]\n",
      "Learning rate :  0.5\n",
      "Final estimates of b and q are:  -0.6722209174123533 [-1.72231896 -1.12645876 -0.4944461  -0.08816491]\n",
      "Number of Iterations :  30\n",
      "Accuracy :  0.8581818181818182\n",
      "Precision :  1.0 \n",
      "recall : 0.8581818181818182\n",
      "F1_score : 0.923679060665362\n",
      "//////////////////////////// GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.3328418532324779 [ 0.71520147  1.67586567  0.01901065 -0.86516996]\n",
      "Learning rate :  1\n",
      "Final estimates of b and q are:  -4.521147667370032 [ -9.50718998 -10.30423963  -9.43380818   0.07603217]\n",
      "Number of Iterations :  5411\n",
      "Accuracy :  0.9927272727272727\n",
      "Precision :  1.0 \n",
      "recall : 0.9927272727272727\n",
      "F1_score : 0.9963503649635036\n",
      "//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -0.5874179293576627 [-1.2654546   0.33668271 -1.73200958 -0.42484175]\n",
      "Learning rate :  1\n",
      "Final estimates of b and q are:  -0.7693257361737661 [-2.33445427 -1.43637451 -1.23883725  0.26152724]\n",
      "Number of Iterations :  15\n",
      "Accuracy :  0.9309090909090909\n",
      "Precision :  1.0 \n",
      "recall : 0.9309090909090909\n",
      "F1_score : 0.9642184557438794\n",
      "//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.7880535615515593 [0.0128186  1.71209817 0.65559006 0.34661193]\n",
      "Learning rate :  1\n",
      "Final estimates of b and q are:  -0.7814473891278622 [-2.17861844 -0.72142002 -0.48901041  0.37641089]\n",
      "Number of Iterations :  22\n",
      "Accuracy :  0.8436363636363636\n",
      "Precision :  1.0 \n",
      "recall : 0.8436363636363636\n",
      "F1_score : 0.9151873767258382\n",
      "//////////////////////////// GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.5792134406304389 [-0.08642612  1.38948408  0.11616655  0.1030903 ]\n",
      "Learning rate :  10\n",
      "Final estimates of b and q are:  -5.728758258893681 [-11.74138076 -12.7370877  -11.70378042  -0.07060037]\n",
      "Number of Iterations :  1172\n",
      "Accuracy :  0.9927272727272727\n",
      "Precision :  1.0 \n",
      "recall : 0.9927272727272727\n",
      "F1_score : 0.9963503649635036\n",
      "//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -1.350285404453076 [-2.00385648 -0.19953422  0.66735417 -0.72868845]\n",
      "Learning rate :  10\n",
      "Final estimates of b and q are:  -0.9280644517768811 [-3.14188922 -2.26613179 -1.99752941  0.36836675]\n",
      "Number of Iterations :  5\n",
      "Accuracy :  0.96\n",
      "Precision :  1.0 \n",
      "recall : 0.96\n",
      "F1_score : 0.9795918367346939\n",
      "//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -0.5036706135891797 [ 0.01948016  0.90965887 -0.64620661  1.71228055]\n",
      "Learning rate :  10\n",
      "Final estimates of b and q are:  -1.209874289129864 [-4.13275233 -2.50912442 -2.47509202  0.42813303]\n",
      "Number of Iterations :  6\n",
      "Accuracy :  0.96\n",
      "Precision :  1.0 \n",
      "recall : 0.96\n",
      "F1_score : 0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////// GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.8419217235196447 [-1.73701678 -0.0735633   0.82500014]\n",
      "Learning rate :  0.01\n",
      "Final estimates of b and q are:  -1.6543467811933958 [-4.49099101 -4.61158954 -4.08804156]\n",
      "Number of Iterations :  31796\n",
      "Precision :  1.0 \n",
      "recall : 0.9927272727272727\n",
      "F1_score : 0.9963503649635036\n",
      "//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -0.10364652668178236 [ 0.91693709  2.19644832 -1.82986784]\n",
      "Learning rate :  0.01\n",
      "Final estimates of b and q are:  -0.7663571963169714 [-2.04167507 -1.30859548 -0.88305629]\n",
      "Number of Iterations :  2193\n",
      "Precision :  1.0 \n",
      "recall : 0.8981818181818182\n",
      "F1_score : 0.946360153256705\n",
      "//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.1649562100444992 [ 0.73856037 -0.352151   -0.66534688]\n",
      "Learning rate :  0.01\n",
      "Final estimates of b and q are:  -0.6135496605179622 [-1.40663001 -1.19241293 -0.69693353]\n",
      "Number of Iterations :  1068\n",
      "Precision :  1.0 \n",
      "recall : 0.8945454545454545\n",
      "F1_score : 0.944337811900192\n",
      "//////////////////////////// GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -1.196608865381739 [-0.33235601  0.2094491   1.6562893 ]\n",
      "Learning rate :  0.5\n",
      "Final estimates of b and q are:  -4.0887505455637365 [-8.64587346 -9.4937452  -8.62506273]\n",
      "Number of Iterations :  7606\n",
      "Precision :  1.0 \n",
      "recall : 0.9927272727272727\n",
      "F1_score : 0.9963503649635036\n",
      "//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.0756941028668582 [-0.62151155 -0.21523359  0.85922981]\n",
      "Learning rate :  0.5\n",
      "Final estimates of b and q are:  -0.7814449045683765 [-2.27652909 -1.48829268 -1.05234965]\n",
      "Number of Iterations :  56\n",
      "Precision :  1.0 \n",
      "recall : 0.9163636363636364\n",
      "F1_score : 0.9563567362428842\n",
      "//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.05735995667435007 [-1.4661557  -0.01710331  1.5182147 ]\n",
      "Learning rate :  0.5\n",
      "Final estimates of b and q are:  -0.7549399111671925 [-2.1502103  -0.97538757 -0.43509856]\n",
      "Number of Iterations :  38\n",
      "Precision :  1.0 \n",
      "recall : 0.84\n",
      "F1_score : 0.9130434782608696\n",
      "//////////////////////////// GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -0.6420255354166001 [ 1.54721868  0.35461286 -0.629695  ]\n",
      "Learning rate :  1\n",
      "Final estimates of b and q are:  -4.582657570165186 [ -9.56291302 -10.45050797  -9.53650219]\n",
      "Number of Iterations :  5444\n",
      "Precision :  1.0 \n",
      "recall : 0.9927272727272727\n",
      "F1_score : 0.9963503649635036\n",
      "//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  1.0085075803966548 [-0.29811404  1.46212998 -0.39923195]\n",
      "Learning rate :  1\n",
      "Final estimates of b and q are:  -0.7946435879095552 [-2.38064795 -1.56670911 -1.14773041]\n",
      "Number of Iterations :  31\n",
      "Precision :  1.0 \n",
      "recall : 0.9236363636363636\n",
      "F1_score : 0.9603024574669187\n",
      "//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.10831636993498418 [ 0.32044081 -0.74891367  1.35108313]\n",
      "Learning rate :  1\n",
      "Final estimates of b and q are:  -0.7176724474124512 [-1.78377757 -1.01578942 -0.47168377]\n",
      "Number of Iterations :  17\n",
      "Precision :  1.0 \n",
      "recall : 0.8472727272727273\n",
      "F1_score : 0.9173228346456693\n",
      "//////////////////////////// GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.16283581103152328 [ 0.7190438   0.34760887 -1.54660664]\n",
      "Learning rate :  10\n",
      "Final estimates of b and q are:  -5.800616046711654 [-11.91882179 -12.86228245 -11.84727637]\n",
      "Number of Iterations :  1208\n",
      "Precision :  1.0 \n",
      "recall : 0.9927272727272727\n",
      "F1_score : 0.9963503649635036\n",
      "//////////////////////////// L1 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  0.8451550162765483 [-1.06194107  0.07107868  0.14940953]\n",
      "Learning rate :  10\n",
      "Final estimates of b and q are:  -1.029817921310073 [-3.27597635 -2.22857126 -1.88425813]\n",
      "Number of Iterations :  4\n",
      "Precision :  1.0 \n",
      "recall : 0.9563636363636364\n",
      "F1_score : 0.9776951672862453\n",
      "//////////////////////////// L2 REGULARIZED GRADIENT   DESCENT///////////////////////////\n",
      "initial guess of b and w:  -0.6595322933333969 [ 0.18389341  1.79147136 -1.19685009]\n",
      "Learning rate :  10\n",
      "Final estimates of b and q are:  -1.311885360639105 [-4.2817947  -2.85908041 -2.51226643]\n",
      "Number of Iterations :  6\n",
      "Precision :  1.0 \n",
      "recall : 0.9636363636363636\n",
      "F1_score : 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "features_main() # 4th attribute weight is almost zero in many cases,remove 4th attribute as it isn't that important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
